{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-K9kchMgJDD"
      },
      "source": [
        "## SIMCLR Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXNcTaPBHYjd"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9VwzxODvuoOV"
      },
      "outputs": [],
      "source": [
        "# !pip3 freeze > requirements.txt\n",
        "# !pip install torchlars\n",
        "# ! pip install lightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVuQrFafuLS4"
      },
      "source": [
        "Referenced from: https://github.com/leftthomas/SimCLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6n1nTJyBgJDK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/newenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# import necessary dependencies\n",
        "import argparse\n",
        "import os, sys\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# import shutil, time, os, requests, random, copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "# from torchlars import LARS\n",
        "from lightly.loss import NTXentLoss\n",
        "\n",
        "# move one directory up to import from src\n",
        "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
        "    os.chdir(\"..\")\n",
        "\n",
        "# from src.setdevice import set_device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBZIPqbXHYjf"
      },
      "source": [
        "### Set Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SskcaVBxHYjf"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "set_seed()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY7ItRwLZekU",
        "outputId": "6b4d7d0a-96ae-4c3b-9945-1a17c6f86b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cuda GPU: Tesla V100-PCIE-16GB\n"
          ]
        }
      ],
      "source": [
        "def set_device():\n",
        "    if torch.cuda.is_available():  # check if NVIDIA GPU is available\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"Running on {device} GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    elif torch.backends.mps.is_available():  # check if Apple's Metal is available\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\n",
        "            f\"Checking pytorch is built with mps activated: {torch.backends.mps.is_built()}\"\n",
        "        )\n",
        "        print(f\"Running on {device} GPU...\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"Running on CPU...\")\n",
        "\n",
        "    return device\n",
        "\n",
        "\n",
        "device = set_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs8eB9yNHYjn"
      },
      "source": [
        "## MOVE TO SCRIPT\n",
        "\n",
        "## Define f(·) encoder - ResNet-20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fsohg9aQHYjn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "It is the neural network model base encoder f(·) that extracts representation vectors \n",
        "from augmented data examples. We choose to use the ResNet20 architecture.\n",
        "\"\"\"\n",
        "\n",
        "# Residual Block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=inplanes,\n",
        "            out_channels=planes,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, out_channels=planes, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        if inplanes > planes or stride > 1:\n",
        "            # Option B\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=inplanes,\n",
        "                    out_channels=planes,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    padding=0,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity if self.downsample is None else self.downsample(identity)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv0 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=self.inplanes,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn0 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.layer1 = self._make_layer(block, planes=16, blocks=layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, planes=32, blocks=layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, planes=64, blocks=layers[2], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride):\n",
        "        layers = []\n",
        "        for i in range(blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    inplanes=(self.inplanes if i == 0 else planes),\n",
        "                    planes=planes,\n",
        "                    stride=stride if i == 0 else 1,\n",
        "                )\n",
        "            )\n",
        "        self.inplanes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn0(self.conv0(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Pj_2ZMHYjo"
      },
      "source": [
        "### Define g(·) projection head - MLP with linear layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EyLEMqiqHYjo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "It is the neural network projection head g(·) that maps representations to \n",
        "the space where contrastive loss is applied. There is the option to use a linear\n",
        "projection head or a nonlinear projection head (2 layer MLP).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, use_bias=False, use_bn=False):\n",
        "        super(LinearLayer, self).__init__()\n",
        "\n",
        "        self.use_bn = use_bn\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=in_features,\n",
        "            out_features=out_features,\n",
        "            bias=use_bias and not use_bn,\n",
        "        )\n",
        "        self.bn = nn.BatchNorm1d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x) if self.use_bn else x\n",
        "        return x\n",
        "\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        hidden_features,\n",
        "        out_features,\n",
        "        head_type=\"nonlinear\",\n",
        "        use_bn=True,\n",
        "    ):\n",
        "        super(ProjectionHead, self).__init__()\n",
        "\n",
        "        if head_type == \"linear\":\n",
        "            self.layers = LinearLayer(\n",
        "                in_features=in_features,\n",
        "                out_features=out_features,\n",
        "                use_bias=False,\n",
        "                use_bn=True,\n",
        "            )\n",
        "        elif head_type == \"nonlinear\":\n",
        "            self.layers = nn.Sequential(\n",
        "                LinearLayer(\n",
        "                    in_features=in_features,\n",
        "                    out_features=hidden_features,\n",
        "                    use_bias=True,\n",
        "                    use_bn=use_bn,\n",
        "                ),\n",
        "                nn.ReLU(),\n",
        "                LinearLayer(\n",
        "                    in_features=hidden_features,\n",
        "                    out_features=out_features,\n",
        "                    use_bias=False,\n",
        "                    use_bn=use_bn,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlxcCfxkHYjp"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iWgSWhf3HYjp"
      },
      "outputs": [],
      "source": [
        "class SimCLR(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates SimCLR model given encoder\n",
        "    Args:\n",
        "      encoder (nn.Module): Encoder\n",
        "      projection_n_in (int): Number of input features of the projection head\n",
        "      projection_n_hidden (int): Number of hidden features of the projection head\n",
        "      projection_n_out (int): Number of output features of the projection head\n",
        "      projection_use_bn (bool): Whether to use batch norm in the projection head\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: nn.Module = ResNet(BasicBlock, [3, 3, 3]),\n",
        "        projection_n_in: int = 64,  # TODO CHECK ON THIS\n",
        "        projection_n_hidden: int = 64,  # TODO CHECK ON THIS\n",
        "        projection_n_out: int = 128,\n",
        "        projection_use_bn: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = ResNet(BasicBlock, [3, 3, 3])\n",
        "        self.projection = ProjectionHead(\n",
        "            in_features=projection_n_in,\n",
        "            hidden_features=projection_n_hidden,\n",
        "            out_features=projection_n_out,\n",
        "            head_type=\"nonlinear\",\n",
        "            use_bn=projection_use_bn,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.encoder(x)\n",
        "        x = self.projection(feature)\n",
        "        return F.normalize(feature, dim=-1), x\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instantiate Model on Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mz1G3CktHYjp"
      },
      "outputs": [],
      "source": [
        "model = SimCLR().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2z2H-YWYrEZ"
      },
      "source": [
        "### Setting up hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po1FyPv2Yt8U",
        "outputId": "8bc56bf5-0720-4d56-9ab9-bca5d063f6c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 6.0000e-03.\n"
          ]
        }
      ],
      "source": [
        "# Tunable hyperparameters\n",
        "LRS = [0.5, 1, 1.5]\n",
        "TEMPS = [0.1, 0.5, 1.0]\n",
        "BATCHES = [256, 512, 1024, 2048, 4096]\n",
        "\n",
        "# for now\n",
        "# LR = 0.5\n",
        "TEMP = 0.5\n",
        "BATCH = 512\n",
        "LR = 0.003 * BATCH / 256\n",
        "EPOCHS = 100\n",
        "DECAY = 1e-4\n",
        "\n",
        "#############################################\n",
        "# loss function\n",
        "criterion = NTXentLoss(temperature=TEMP)\n",
        "\n",
        "# Add optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=DECAY)\n",
        "\n",
        "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
        "# SCHEDULER OR LINEAR WARMUP\n",
        "# warmupscheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "#     optimizer, lambda epoch: (epoch + 1) / 10.0, verbose=True\n",
        "# )\n",
        "\n",
        "# # SCHEDULER FOR COSINE DECAY\n",
        "# mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "#     optimizer, len(data_load), eta_min=0.05, last_epoch=-1, verbose=True\n",
        "# )\n",
        "\n",
        "# SCHEDULER FOR COSINE DECAY\n",
        "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, 50_000, verbose=True\n",
        ")\n",
        "#############################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_On6NO4HYjg"
      },
      "source": [
        "### Pre-processing functions for augmenting images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UeDui5ckYMgu"
      },
      "outputs": [],
      "source": [
        "s = 0.5\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(\n",
        "            (32, 32),\n",
        "            scale=(0.08, 1.0),\n",
        "            ratio=(0.75, 1.3333333333333333),\n",
        "        ),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply(\n",
        "            [transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)], p=0.8\n",
        "        ),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        # transforms.RandomApply([transforms.GaussianBlur(kernel_size = 3, sigma=(0.1, 2.0))], p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0iqhq7bGJvz"
      },
      "source": [
        "### Setting up dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxbZ9jIGFWn",
        "outputId": "0143459d-970b-4f0a-ccd5-2dc18c6b254d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 83046107.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Alternative class for dataloader\n",
        "class getC10Pair(CIFAR10):\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img, label = self.data[idx], self.targets[idx]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img)\n",
        "            img2 = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return img1, img2, label\n",
        "\n",
        "\n",
        "train_data = getC10Pair(\n",
        "    root=\"data\", train=True, transform=train_transform, download=True\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_data, batch_size=BATCH, shuffle=True, num_workers=4, drop_last=True\n",
        ")\n",
        "\n",
        "memory_data = getC10Pair(\n",
        "    root=\"data\", train=True, transform=test_transform, download=True\n",
        ")\n",
        "memory_loader = DataLoader(memory_data, batch_size=BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "test_data = getC10Pair(\n",
        "    root=\"data\", train=False, transform=test_transform, download=True\n",
        ")\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYF0w9pZHYjq"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTk06OndcdU7"
      },
      "outputs": [],
      "source": [
        "# def save_model(model, optimizer, scheduler, current_epoch, name):\n",
        "#   out = os.path.join('/workspaces/RepLearning/saved_models/',name.format(current_epoch))\n",
        "\n",
        "#   torch.save({'model_state_dict': model.state_dict(),\n",
        "#               'optimizer_state_dict': optimizer.state_dict(),\n",
        "#               'scheduler_state_dict':scheduler.state_dict()}, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iD5l5mzRHYjq",
        "outputId": "bcf54ebe-7f0b-41cd-e4f6-5c491ac72733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# alternative\n",
        "current_epoch = 0\n",
        "\n",
        "DECAY_EPOCHS = 10\n",
        "\n",
        "results = {\"train_loss\": [], \"test_acc@1\": [], \"test_acc@5\": []}\n",
        "save_name_pre = \"B{}_E{}\".format(BATCH, EPOCHS)\n",
        "\n",
        "if not os.path.exists(\"./results\"):\n",
        "    os.mkdir(\"results\")\n",
        "\n",
        "if not os.path.exists(\"./results/backup\"):\n",
        "    os.mkdir(\"results/backup\")\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(\"\")\n",
        "    model.train()\n",
        "    total_loss, total_num, train_bar = 0.0, 0, tqdm(train_loader)\n",
        "\n",
        "    for x_1, x_2, target in train_bar:\n",
        "        x_1, x_2 = x_1.cuda(non_blocking=True), x_2.cuda(non_blocking=True)\n",
        "\n",
        "        feature_1, out_1 = model(x_1)\n",
        "        feature_2, out_2 = model(x_2)\n",
        "\n",
        "        loss = criterion(out_1, out_2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_num += BATCH\n",
        "        total_loss += loss.item() * BATCH\n",
        "        train_bar.set_description(\n",
        "            \"Train Epoch: [{}/{}] Loss: {:.4f}\".format(\n",
        "                epoch + 1, EPOCHS, total_loss / total_num\n",
        "            )\n",
        "        )\n",
        "\n",
        "        train_loss = total_loss / total_num\n",
        "\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "\n",
        "    # if epoch < 10:\n",
        "    #     warmupscheduler.step()\n",
        "    if epoch >= 10:\n",
        "        mainscheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # generate feature bank\n",
        "        for data, _, target in tqdm(memory_loader, desc=\"Feature extracting\"):\n",
        "            feature, out = model(data.cuda(non_blocking=True))\n",
        "            feature_bank.append(feature)\n",
        "        # [D, N]\n",
        "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
        "        # [N]\n",
        "        feature_labels = torch.tensor(\n",
        "            memory_loader.dataset.targets, device=feature_bank.device\n",
        "        )\n",
        "\n",
        "        # loop test data to predict the label by weighted knn search\n",
        "        test_bar = tqdm(test_loader)\n",
        "        for data, _, target in test_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            feature, out = model(data)\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "            sim_matrix = torch.mm(feature, feature_bank)\n",
        "            # [B, K]\n",
        "            sim_weight, sim_indices = sim_matrix.topk(k=200, dim=-1)\n",
        "            # [B, K]\n",
        "            sim_labels = torch.gather(\n",
        "                feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices\n",
        "            )\n",
        "            sim_weight = (sim_weight / TEMP).exp()\n",
        "\n",
        "            # counts for each class\n",
        "            one_hot_label = torch.zeros(\n",
        "                data.size(0) * 200, 10, device=sim_labels.device\n",
        "            )\n",
        "            # [B*K, C]\n",
        "            one_hot_label = one_hot_label.scatter(\n",
        "                dim=-1, index=sim_labels.view(-1, 1), value=1.0\n",
        "            )\n",
        "            # weighted score ---> [B, C]\n",
        "            pred_scores = torch.sum(\n",
        "                one_hot_label.view(data.size(0), -1, 10) * sim_weight.unsqueeze(dim=-1),\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "\n",
        "            total_top1 += torch.sum(\n",
        "                (pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
        "            ).item()\n",
        "            total_top5 += torch.sum(\n",
        "                (pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
        "            ).item()\n",
        "            test_bar.set_description(\n",
        "                \"Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%\".format(\n",
        "                    epoch + 1,\n",
        "                    EPOCHS,\n",
        "                    total_top1 / total_num * 100,\n",
        "                    total_top5 / total_num * 100,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    test_acc_1 = total_top1 / total_num * 100\n",
        "    test_acc_5 = total_top5 / total_num * 100\n",
        "\n",
        "    results[\"test_acc@1\"].append(test_acc_1)\n",
        "    results[\"test_acc@5\"].append(test_acc_5)\n",
        "\n",
        "\n",
        "\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "        # torch.save(model.state_dict(), '/workspaces/RepLearning/saved_models/{}_best_model.pth'.format(save_name_pre))\n",
        "        torch.save(\n",
        "            model.state_dict(), \"./results/{}_best_model.pth\".format(save_name_pre)\n",
        "        )\n",
        "        # save statistics\n",
        "        data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "        data_frame.to_csv('results/{}_statistics.csv'.format(save_name_pre), index_label='epoch')\n",
        "\n",
        "    # save_model(model, optimizer, mainscheduler, current_epoch, \"SimCLR_CIFAR10_RN20_bestacc.pt\")\n",
        "    # save every epoch as backup\n",
        "    torch.save(model.state_dict(), f\"./results/backup/{save_name_pre}_e{epoch}.pth\")\n",
        "\n",
        "    # current_epoch+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XPkyImcSnBM"
      },
      "outputs": [],
      "source": [
        "# def plot_features(model, num_classes, num_feats, batch_size):\n",
        "#     preds = np.array([]).reshape((0,1))\n",
        "#     gt = np.array([]).reshape((0,1))\n",
        "#     feats = np.array([]).reshape((0,num_feats))\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for x1,_, target in test_loader:\n",
        "#             x1 = x1.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
        "#             target = target.squeeze().to(device = 'cuda:0')\n",
        "#             out = model(x1)\n",
        "#             out = out.detach().to(\"cpu\").data.numpy()#.reshape((1,-1))\n",
        "#             feats = np.append(feats,out,axis = 0)\n",
        "\n",
        "#             # feature_labels = torch.tensor(memory_loader.dataset.targets, device=feature_bank.device)\n",
        "\n",
        "#     tsne = TSNE(n_components = 2, perplexity = 50)\n",
        "#     x_feats = tsne.fit_transform(feats)\n",
        "#     num_samples = int(batch_size*(test_data.shape[0]//batch_size))#(len(val_df)\n",
        "\n",
        "#     for i in range(num_classes):\n",
        "#         plt.scatter(x_feats[target[:num_samples]==i,1],x_feats[target[:num_samples]==i,0])\n",
        "\n",
        "#     plt.legend([str(i) for i in range(num_classes)])\n",
        "#     plt.show()\n",
        "\n",
        "# plot_features(model, 10, 64, 512)\n",
        "\n",
        "# # torch.save(model.state_dict(), './results/{}_best_model.pth'.format(save_name_pre))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l-xEYAy0k5FU"
      },
      "source": [
        "### Linear Evaluations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load a model\n",
        "Careful to only use the encoder part of the model to evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvalNet(nn.Module):\n",
        "    def __init__(self, num_class, pretrained_path):\n",
        "        super(EvalNet, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.encoder = ResNet(BasicBlock, [3, 3, 3])\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(64, 10, bias=True)\n",
        "        self.load_state_dict(\n",
        "            torch.load(pretrained_path, map_location=\"cpu\"), strict=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        out = self.fc(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load best model\n",
        "# model_path = '/workspaces/RepLearning/saved_models/{}_best_model.pth'.format(save_name_pre)\n",
        "model_path = \"results/B256_E1_best_model.pth\"\n",
        "\n",
        "model = EvalNet(num_class=10, pretrained_path=model_path).to(device)\n",
        "\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "eval_LRATE = 0.001 * BATCH / 256\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=eval_LRATE, weight_decay=1e-4) \n",
        "# optimizer = optim.SGD(model.fc.parameters(), lr=eval_LRATE, momentum=0.9, nesterov=True)\n",
        "criterion = nn.CrossEntropyLoss() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CyL6SmW5OAfS"
      },
      "outputs": [],
      "source": [
        "eval_train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(\n",
        "            (32, 32),\n",
        "            scale=(0.08, 1.0),\n",
        "            ratio=(0.75, 1.3333333333333333),\n",
        "        ),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # transforms.RandomApply(\n",
        "        #     [transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)], p=0.8\n",
        "        # ),\n",
        "        # transforms.RandomGrayscale(p=0.2),\n",
        "        # transforms.RandomApply([transforms.GaussianBlur(kernel_size = 3, sigma=(0.1, 2.0))], p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspaces/DRLrecommenders/dnn'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/newenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "num_imgs_per_category 500\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from dataloader_eval import DataLoader, GenericDataset\n",
        "\n",
        "train_set = GenericDataset(\n",
        "    dataset_name='cifar10',\n",
        "    split= 'train', # crop and horizontal flip \n",
        "    num_imgs_per_cat=500) # 10% of the training set\n",
        "val_set = GenericDataset(\n",
        "    dataset_name='cifar10',\n",
        "    split='test') \n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=128,\n",
        "    unsupervised=False,\n",
        "    epoch_size=10*5000,\n",
        "    num_workers=4,\n",
        "    shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_set,\n",
        "    batch_size=128,\n",
        "    unsupervised=False,\n",
        "    epoch_size=None,\n",
        "    num_workers=4,\n",
        "    shuffle=False)\n",
        "\n",
        "############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV6jsrPwk4zA",
        "outputId": "15297d9f-4b91-401d-d5a5-b647bcd58487"
      },
      "outputs": [],
      "source": [
        "# train_data = CIFAR10(\n",
        "#     root=\"data\", train=True, transform=eval_train_transform, download=True\n",
        "# )\n",
        "# train_loader = DataLoader(train_data, batch_size=BATCH, shuffle=True, num_workers=4)\n",
        "\n",
        "# test_data = CIFAR10(\n",
        "#     root=\"data\", train=False, transform=eval_test_transform, download=True\n",
        "# )\n",
        "# test_loader = DataLoader(test_data, batch_size=BATCH, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uqid_b8Vk4wU"
      },
      "outputs": [],
      "source": [
        "# # load best model\n",
        "# # model_path = '/workspaces/RepLearning/saved_models/{}_best_model.pth'.format(save_name_pre)\n",
        "# model_path = \"./results/{}_best_model.pth\".format(save_name_pre)\n",
        "\n",
        "# model = EvalNet(num_class=10, pretrained_path=model_path).to(device)\n",
        "\n",
        "# for param in model.encoder.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# eval_LRATE = 0.1 * BATCH / 256\n",
        "# optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6) #TODO verify this\n",
        "# # optimizer = optim.SGD(model.fc.parameters(), lr=eval_LRATE, momentum=0.9, nesterov=True)\n",
        "# criterion = nn.CrossEntropyLoss()  # TODO verify this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cifar_dataset = torchvision.datasets.CIFAR10(root='./data', transform=transform)\n",
        "# train_indices = # select train indices according to your rule\n",
        "# test_indices = # select test indices according to your rule\n",
        "# train_loader = torch.utils.data.DataLoader(cifar_dataset, batch_size=32, shuffle=True, sampler=SubsetRandomSampler(train_indices))\n",
        "# test_loader = torch.utils.data.DataLoader(cifar_dataset, batch_size=32, shuffle=True, sampler=SubsetRandomSampler(test_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bcf737Pk4ti",
        "outputId": "02d99144-1d0a-4744-8fde-d3d4d48ec164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/391 [00:00<?, ?it/s]/opt/conda/envs/newenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975993/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/opt/conda/envs/newenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975993/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/opt/conda/envs/newenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975993/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/opt/conda/envs/newenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975993/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "results = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc@1\": [],\n",
        "    \"train_acc@5\": [],\n",
        "    \"test_loss\": [],\n",
        "    \"test_acc@1\": [],\n",
        "    \"test_acc@5\": [],\n",
        "}\n",
        "\n",
        "best_acc = 0.0\n",
        "\n",
        "EVAL_EPOCHS = 30\n",
        "\n",
        "for epoch in range(1, EVAL_EPOCHS + 1):\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss, total_correct_1, total_correct_5, total_num, data_bar = (\n",
        "        0.0,\n",
        "        0.0,\n",
        "        0.0,\n",
        "        0,\n",
        "        tqdm(train_loader(0)),\n",
        "    )\n",
        "    with torch.enable_grad():\n",
        "        for data, target in data_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "\n",
        "            out = model(data)\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "\n",
        "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
        "            total_correct_1 += torch.sum(\n",
        "                (prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
        "            ).item()\n",
        "            total_correct_5 += torch.sum(\n",
        "                (prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
        "            ).item()\n",
        "\n",
        "            data_bar.set_description(\n",
        "                \"{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%\".format(\n",
        "                    \"Train\",\n",
        "                    epoch,\n",
        "                    EVAL_EPOCHS,\n",
        "                    total_loss / total_num,\n",
        "                    total_correct_1 / total_num * 100,\n",
        "                    total_correct_5 / total_num * 100,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    train_loss = total_loss / total_num\n",
        "    train_acc_1 = total_correct_1 / total_num * 100\n",
        "    train_acc_5 = total_correct_5 / total_num * 100\n",
        "\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc@1\"].append(train_acc_1)\n",
        "    results[\"train_acc@5\"].append(train_acc_5)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_correct_1, total_correct_5, total_num, data_bar = (\n",
        "        0.0,\n",
        "        0.0,\n",
        "        0.0,\n",
        "        0,\n",
        "        tqdm(test_loader(0)),\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            out = model(data)\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "\n",
        "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
        "            total_correct_1 += torch.sum(\n",
        "                (prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
        "            ).item()\n",
        "            total_correct_5 += torch.sum(\n",
        "                (prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()\n",
        "            ).item()\n",
        "\n",
        "            data_bar.set_description(\n",
        "                \"{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%\".format(\n",
        "                    \"Test\",\n",
        "                    epoch,\n",
        "                    EVAL_EPOCHS,\n",
        "                    total_loss / total_num,\n",
        "                    total_correct_1 / total_num * 100,\n",
        "                    total_correct_5 / total_num * 100,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    test_loss = total_loss / total_num\n",
        "    test_acc_1 = total_correct_1 / total_num * 100\n",
        "    test_acc_5 = total_correct_5 / total_num * 100\n",
        "\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc@1\"].append(test_acc_1)\n",
        "    results[\"test_acc@5\"].append(test_acc_5)\n",
        "\n",
        "    # # save statistics\n",
        "    # data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "    # data_frame.to_csv('results/linear_statistics.csv', index_label='epoch')\n",
        "\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "        # torch.save(model.state_dict(), '/workspaces/RepLearning/saved_models/linear_model.pth')\n",
        "        torch.save(model.state_dict(), \"./results/linear_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOGvbcEJpQ9-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPpqG_lHk4qj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LauO2s0vk4lJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyEYYGMyk4ik"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZfFKARuk4gF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2wGoIx0k4dx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjviwWUek4bH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khm3cnRUk4Vq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7345wLek4TP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "idxkp4Enk4QR"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3780048248.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/7y/c5pg88dn299dxzx9v5hs68qw0000gn/T/ipykernel_30716/3780048248.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ---------------------------\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8JKXa4yHYjr"
      },
      "source": [
        "dg = C10DataGen('train',trimages)\n",
        "dl = DataLoader(dg,batch_size = 128,drop_last=True)\n",
        "\n",
        "vdg = C10DataGen('valid',valimages)\n",
        "vdl = DataLoader(vdg,batch_size = 128,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDow7PU5HYjr",
        "outputId": "1b1dafc8-d229-41c9-9055-bf5db44e6555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/5]\t\n",
            "Step [0/156]\t Loss: 8.21142\n",
            "Step [50/156]\t Loss: 6.96044\n",
            "Step [100/156]\t Loss: 6.46314\n",
            "Step [150/156]\t Loss: 5.86387\n",
            "Adjusting learning rate of group 0 to 1.0000e-01.\n",
            "Step [0/39]\t Loss: 0.38851\n",
            "Epoch [0/5]\t Training Loss: 6.600502879191668\t lr: 0.1\n",
            "Epoch [0/5]\t Validation Loss: 0.39332603491269624\t lr: 0.1\n",
            "Epoch [0/5]\t Time Taken: 2.455996863047282 minutes\n",
            "Epoch [1/5]\t\n",
            "Step [0/156]\t Loss: 6.24187\n",
            "Step [50/156]\t Loss: 6.18044\n",
            "Step [100/156]\t Loss: 5.93262\n",
            "Step [150/156]\t Loss: 6.08959\n",
            "Adjusting learning rate of group 0 to 1.5000e-01.\n",
            "Step [0/39]\t Loss: 0.40206\n",
            "Epoch [1/5]\t Training Loss: 6.109975503041194\t lr: 0.15\n",
            "Epoch [1/5]\t Validation Loss: 0.41858119001755345\t lr: 0.15\n",
            "Epoch [1/5]\t Time Taken: 2.735532263914744 minutes\n",
            "Epoch [2/5]\t\n",
            "Step [0/156]\t Loss: 6.10084\n",
            "Step [50/156]\t Loss: 6.20254\n",
            "Step [100/156]\t Loss: 6.03884\n",
            "Step [150/156]\t Loss: 6.13484\n",
            "Adjusting learning rate of group 0 to 2.0000e-01.\n",
            "Step [0/39]\t Loss: 0.42708\n",
            "Epoch [2/5]\t Training Loss: 6.014321721517122\t lr: 0.2\n",
            "Epoch [2/5]\t Validation Loss: 0.4344389247588622\t lr: 0.2\n",
            "Epoch [2/5]\t Time Taken: 2.4550832708676658 minutes\n",
            "Epoch [3/5]\t\n",
            "Step [0/156]\t Loss: 6.09048\n",
            "Step [50/156]\t Loss: 6.02138\n",
            "Step [100/156]\t Loss: 6.03997\n",
            "Step [150/156]\t Loss: 5.92063\n",
            "Adjusting learning rate of group 0 to 2.5000e-01.\n",
            "Step [0/39]\t Loss: 0.4221\n",
            "Epoch [3/5]\t Training Loss: 5.898574905517774\t lr: 0.25\n",
            "Epoch [3/5]\t Validation Loss: 0.44937508610578686\t lr: 0.25\n",
            "Epoch [3/5]\t Time Taken: 2.4601907690366107 minutes\n",
            "Epoch [4/5]\t\n",
            "Step [0/156]\t Loss: 5.847\n",
            "Step [50/156]\t Loss: 5.60829\n",
            "Step [100/156]\t Loss: 5.92956\n",
            "Step [150/156]\t Loss: 5.80584\n",
            "Adjusting learning rate of group 0 to 3.0000e-01.\n",
            "Step [0/39]\t Loss: 0.44297\n",
            "Epoch [4/5]\t Training Loss: 5.766448519168756\t lr: 0.3\n",
            "Epoch [4/5]\t Validation Loss: 0.46796969572703045\t lr: 0.3\n",
            "Epoch [4/5]\t Time Taken: 2.4485480388005576 minutes\n",
            "Epoch [5/5]\t\n",
            "Step [0/156]\t Loss: 5.96158\n",
            "Step [50/156]\t Loss: 5.65614\n",
            "Step [100/156]\t Loss: 5.5403\n",
            "Step [150/156]\t Loss: 5.61173\n",
            "Adjusting learning rate of group 0 to 3.5000e-01.\n",
            "Step [0/39]\t Loss: 0.45664\n",
            "Epoch [5/5]\t Training Loss: 5.616237790156633\t lr: 0.35\n",
            "Epoch [5/5]\t Validation Loss: 0.47664230068524677\t lr: 0.35\n",
            "Epoch [5/5]\t Time Taken: 2.4352718750635782 minutes\n",
            "Epoch [6/5]\t\n",
            "Step [0/156]\t Loss: 5.56861\n",
            "Step [50/156]\t Loss: 5.57918\n",
            "Step [100/156]\t Loss: 5.1119\n",
            "Step [150/156]\t Loss: 5.51921\n",
            "Adjusting learning rate of group 0 to 4.0000e-01.\n",
            "Step [0/39]\t Loss: 0.46369\n",
            "Epoch [6/5]\t Training Loss: 5.483487502122537\t lr: 0.4\n",
            "Epoch [6/5]\t Validation Loss: 0.47679370183211106\t lr: 0.4\n",
            "Epoch [6/5]\t Time Taken: 2.4374259154001874 minutes\n",
            "Epoch [7/5]\t\n",
            "Step [0/156]\t Loss: 5.4154\n",
            "Step [50/156]\t Loss: 5.46581\n",
            "Step [100/156]\t Loss: 5.113\n",
            "Step [150/156]\t Loss: 5.20301\n",
            "Adjusting learning rate of group 0 to 4.5000e-01.\n",
            "Step [0/39]\t Loss: 0.45134\n",
            "Epoch [7/5]\t Training Loss: 5.323172529538472\t lr: 0.45\n",
            "Epoch [7/5]\t Validation Loss: 0.45625446851436907\t lr: 0.45\n",
            "Epoch [7/5]\t Time Taken: 2.4431582848230997 minutes\n",
            "Epoch [8/5]\t\n",
            "Step [0/156]\t Loss: 5.08607\n",
            "Step [50/156]\t Loss: 5.29575\n",
            "Step [100/156]\t Loss: 4.8783\n",
            "Step [150/156]\t Loss: 4.97185\n",
            "Adjusting learning rate of group 0 to 5.0000e-01.\n",
            "Step [0/39]\t Loss: 0.42186\n",
            "Epoch [8/5]\t Training Loss: 5.18836074303358\t lr: 0.5\n",
            "Epoch [8/5]\t Validation Loss: 0.43188346043611187\t lr: 0.5\n",
            "Epoch [8/5]\t Time Taken: 2.444971525669098 minutes\n",
            "Epoch [9/5]\t\n",
            "Step [0/156]\t Loss: 5.16401\n",
            "Step [50/156]\t Loss: 5.48892\n",
            "Step [100/156]\t Loss: 4.94647\n",
            "Step [150/156]\t Loss: 4.83196\n",
            "Adjusting learning rate of group 0 to 5.5000e-01.\n",
            "Step [0/39]\t Loss: 0.4078\n",
            "Epoch [9/5]\t Training Loss: 5.055535041368925\t lr: 0.55\n",
            "Epoch [9/5]\t Validation Loss: 0.42078678348125553\t lr: 0.55\n",
            "Epoch [9/5]\t Time Taken: 2.4414204676946003 minutes\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'SimCLR' object has no attribute 'pretrained'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Time Taken: \u001b[39m\u001b[39m{\u001b[39;00mtime_taken\u001b[39m}\u001b[39;00m\u001b[39m minutes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         plot_features(model\u001b[39m.\u001b[39;49mpretrained, \u001b[39m10\u001b[39m, \u001b[39m2048\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[1;32m     90\u001b[0m save_model(\n\u001b[1;32m     91\u001b[0m     model,\n\u001b[1;32m     92\u001b[0m     optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSimCLR_CIFAR10_RN20_checkpoint_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_test?.pt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SimCLR' object has no attribute 'pretrained'"
          ]
        }
      ],
      "source": [
        "nr = 0\n",
        "current_epoch = 0\n",
        "epochs = 5\n",
        "tr_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
        "    stime = time.time()\n",
        "\n",
        "    model.train()\n",
        "    tr_loss_epoch = 0\n",
        "\n",
        "    for step, (x_i, x_j) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x_i = x_i.squeeze().to(\"cuda:0\").float()\n",
        "        x_j = x_j.squeeze().to(\"cuda:0\").float()\n",
        "\n",
        "        # positive pair, with encoding\n",
        "        z_i = model(x_i)\n",
        "        z_j = model(x_j)\n",
        "\n",
        "        loss = criterion(z_i, z_j)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if nr == 0 and step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {round(loss.item(), 5)}\")\n",
        "\n",
        "        tr_loss_epoch += loss.item()\n",
        "\n",
        "    if nr == 0 and epoch < 10:\n",
        "        warmupscheduler.step()\n",
        "    if nr == 0 and epoch >= 10:\n",
        "        mainscheduler.step()\n",
        "\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    if nr == 0 and (epoch + 1) % 50 == 0:\n",
        "        save_model(\n",
        "            model,\n",
        "            optimizer,\n",
        "            mainscheduler,\n",
        "            current_epoch,\n",
        "            \"SimCLR_CIFAR10_RN20_checkpoint_{}.pt\",\n",
        "        )\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss_epoch = 0\n",
        "        for step, (x_i, x_j) in enumerate(val_loader):\n",
        "\n",
        "            x_i = x_i.squeeze().to(\"cuda:0\").float()\n",
        "            x_j = x_j.squeeze().to(\"cuda:0\").float()\n",
        "\n",
        "            # positive pair, with encoding\n",
        "            z_i = model(x_i)\n",
        "            z_j = model(x_j)\n",
        "\n",
        "            loss = criterion(z_i, z_j)\n",
        "\n",
        "            if nr == 0 and step % 50 == 0:\n",
        "                print(f\"Step [{step}/{len(val_loader)}]\\t Loss: {round(loss.item(),5)}\")\n",
        "\n",
        "            val_loss_epoch += loss.item()\n",
        "\n",
        "    if nr == 0:\n",
        "        tr_loss.append(tr_loss_epoch / len(train_loader))\n",
        "        val_loss.append(val_loss_epoch / len(val_loader))\n",
        "        print(\n",
        "            f\"Epoch [{epoch}/{epochs}]\\t Training Loss: {tr_loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Epoch [{epoch}/{epochs}]\\t Validation Loss: {val_loss_epoch / len(val_loader)}\\t lr: {round(lr, 5)}\"\n",
        "        )\n",
        "        current_epoch += 1\n",
        "\n",
        "    train_dataset.on_epoch_end()\n",
        "\n",
        "    time_taken = (time.time() - stime) / 60\n",
        "    print(f\"Epoch [{epoch}/{epochs}]\\t Time Taken: {time_taken} minutes\")\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        plot_features(model.pretrained, 10, 2048, 128)\n",
        "\n",
        "save_model(\n",
        "    model,\n",
        "    optimizer,\n",
        "    mainscheduler,\n",
        "    current_epoch,\n",
        "    \"SimCLR_CIFAR10_RN20_checkpoint_{}_test?.pt\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDxgzGeWHYjs"
      },
      "outputs": [],
      "source": [
        "# print(\"==> Training starts!\")\n",
        "# print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# for i in range(0, EPOCHS):\n",
        "#     # handle the learning rate scheduler.\n",
        "#     # BONUS\n",
        "#     if i in DECAY_EPOCHS and i != 0:\n",
        "#         current_learning_rate = current_learning_rate * DECAY\n",
        "#         for param_group in optimizer.param_groups:\n",
        "#             param_group[\"lr\"] = current_learning_rate\n",
        "#         print(\"Current learning rate has decayed to %f\" % current_learning_rate)\n",
        "\n",
        "#     #######################\n",
        "#     # your code here\n",
        "#     # switch to train mode\n",
        "#     model.train()\n",
        "\n",
        "#     #######################\n",
        "\n",
        "#     print(\"Epoch %d:\" % i)\n",
        "#     # this help you compute the training accuracy\n",
        "#     total_examples = 0\n",
        "#     correct_examples = 0\n",
        "\n",
        "#     train_loss = 0  # track training loss if you want\n",
        "\n",
        "#     # Train the model for 1 epoch.\n",
        "#     for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "#         ####################################\n",
        "#         # your code here\n",
        "#         # copy inputs to device\n",
        "#         inputs = inputs.to(device)\n",
        "\n",
        "#         # compute the output and loss\n",
        "#         outputs = model(inputs)\n",
        "#         targets = targets.to(device)  # don't forget to also copy targets to device\n",
        "#         loss = criterion(outputs, targets)\n",
        "\n",
        "#         # zero the gradient\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # backpropagation\n",
        "#         loss.backward()\n",
        "\n",
        "#         # apply gradient and update the weights\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # count the number of correctly predicted samples in the current batch\n",
        "#         _, preds = outputs.max(1)\n",
        "#         total_examples += targets.shape[0]\n",
        "#         correct_examples += preds.eq(targets).sum().item()\n",
        "#         # track loss\n",
        "#         train_loss += loss.item()\n",
        "\n",
        "#         ####################################\n",
        "\n",
        "#     avg_loss = train_loss / len(train_loader)\n",
        "#     avg_acc = correct_examples / total_examples\n",
        "#     print(\"Training loss: %.4f, Training accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "\n",
        "#     # store train accuracy and loss\n",
        "#     save_train_acc = avg_acc\n",
        "#     avg_train_loss.append(avg_loss)\n",
        "#     avg_train_acc.append(avg_acc)\n",
        "\n",
        "#     # Validate on the validation dataset\n",
        "#     #######################\n",
        "#     # your code here\n",
        "#     # switch to eval mode\n",
        "#     model.eval()\n",
        "\n",
        "#     #######################\n",
        "\n",
        "#     # this help you compute the validation accuracy\n",
        "#     total_examples = 0\n",
        "#     correct_examples = 0\n",
        "\n",
        "#     val_loss = 0  # again, track the validation loss if you want\n",
        "\n",
        "#     # disable gradient during validation, which can save GPU memory\n",
        "#     with torch.no_grad():\n",
        "#         for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "#             ####################################\n",
        "#             # your code here\n",
        "#             # copy inputs to device\n",
        "#             inputs = inputs.to(device)\n",
        "\n",
        "#             # compute the output and loss\n",
        "#             outputs = model(inputs)\n",
        "#             targets = targets.to(device)  # don't forget to also copy targets to device\n",
        "#             loss = criterion(outputs, targets)\n",
        "\n",
        "#             # count the number of correctly preds samples in the current batch\n",
        "#             _, preds = outputs.max(1)\n",
        "#             total_examples += targets.shape[0]\n",
        "#             correct_examples += preds.eq(targets).sum().item()\n",
        "#             # track loss\n",
        "#             val_loss += loss.item()\n",
        "\n",
        "#             ####################################\n",
        "\n",
        "#     avg_loss = val_loss / len(val_loader)\n",
        "#     avg_acc = correct_examples / total_examples\n",
        "#     print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "\n",
        "#     # store val accuracy and loss\n",
        "#     avg_val_loss.append(avg_loss)\n",
        "#     avg_val_acc.append(avg_acc)\n",
        "\n",
        "#     # save the model checkpoint\n",
        "#     if avg_acc > best_val_acc:\n",
        "#         best_val_acc = avg_acc\n",
        "#         if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "#             os.makedirs(CHECKPOINT_FOLDER)\n",
        "#         print(\"Saving ...\")\n",
        "#         state = {\n",
        "#             \"state_dict\": model.state_dict(),\n",
        "#             \"epoch\": i,\n",
        "#             \"lr\": current_learning_rate,\n",
        "#             \"val_acc\": avg_acc,\n",
        "#             \"train_acc\": save_train_acc,\n",
        "#         }\n",
        "#         torch.save(state, os.path.join(CHECKPOINT_FOLDER, \"resnet.pth\"))\n",
        "\n",
        "# print(\"=\" * 50)\n",
        "# print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUSkq-C0gJDT"
      },
      "outputs": [],
      "source": [
        "# # save/load the training and validation accuracy and loss\n",
        "# SAVE_NEW_DATA = True\n",
        "# if SAVE_NEW_DATA:\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"train_acc.pkl\"), \"wb\") as f:\n",
        "#         pickle.dump(avg_train_acc, f)\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"train_loss.pkl\"), \"wb\") as f:\n",
        "#         pickle.dump(avg_train_loss, f)\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"val_acc.pkl\"), \"wb\") as f:\n",
        "#         pickle.dump(avg_val_acc, f)\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"val_loss.pkl\"), \"wb\") as f:\n",
        "#         pickle.dump(avg_val_loss, f)\n",
        "# else:\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"train_acc.pkl\"), \"rb\") as f:\n",
        "#         avg_train_acc = pickle.load(f)\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"train_loss.pkl\"), \"rb\") as f:\n",
        "#         avg_train_loss = pickle.load(f)\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"val_acc.pkl\"), \"rb\") as f:\n",
        "#         avg_val_acc = pickle.load(f)\n",
        "#     with open(os.path.join(CHECKPOINT_FOLDER, \"val_loss.pkl\"), \"rb\") as f:\n",
        "#         avg_val_loss = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n4gfFo_gJDS"
      },
      "source": [
        "### Save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS7MAxECgJDS",
        "outputId": "58350b13-8d76-4061-d28c-c292c66d1be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data/cifar10_test_F22.zip\n",
            "Extracting ./data/cifar10_test_F22.zip to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# DATA_ROOT = \"./data\"\n",
        "# BATCH_SIZE = 100\n",
        "\n",
        "# transform_test = transforms.Compose(\n",
        "#     [\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# test_set = CIFAR10(root=DATA_ROOT, mode=\"test\", download=True, transform=transform_test)\n",
        "\n",
        "# # do NOT shuffle your test data loader!!!!!!!!!!!!!!!!\n",
        "# # otherwise the order of samples will be messed up\n",
        "# # and your test accuracy is likely to drop to random guessing level\n",
        "# test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
        "\n",
        "# #########################################################\n",
        "# # use your model to generate predictions on test data\n",
        "# # and save the results into variable \"results\"\n",
        "# # \"results\" should be either a numpy array or a torch tensor with length of 10000\n",
        "\n",
        "# # initialize a resnet and load trained weights\n",
        "# net = ResNet(BasicBlock, [3, 3, 3])\n",
        "# state_dict = torch.load(\n",
        "#     os.path.join(CHECKPOINT_FOLDER, \"resnet.pth\")\n",
        "# )  # change the path to your own checkpoint file\n",
        "# net.load_state_dict(state_dict[\"state_dict\"])\n",
        "# net.to(device)\n",
        "\n",
        "# # remember to switch to eval mode whenever you are making inference\n",
        "# net.eval()\n",
        "\n",
        "# results = []\n",
        "# with torch.no_grad():\n",
        "#     for x in test_loader:\n",
        "#         results.append(net(x.to(device)).argmax(1))\n",
        "\n",
        "# # convert results to numpy array\n",
        "# results = torch.cat(results).cpu().numpy()\n",
        "# assert len(results) == 10000\n",
        "\n",
        "# #########################################################\n",
        "# with open(os.path.join(CHECKPOINT_FOLDER, \"predictions.csv\"), \"w\") as fp:\n",
        "#     fp.write(\"Id,Label\\n\")\n",
        "#     for i in range(len(results)):\n",
        "#         fp.write(\"%d,%d\\n\" % (i, results[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDYjRSXvuh1K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4n4gfFo_gJDS"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "newenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "392940e632e2169894237213b912f5d2e3bbd9606ac54fbd3fa76f97bf86cab0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
